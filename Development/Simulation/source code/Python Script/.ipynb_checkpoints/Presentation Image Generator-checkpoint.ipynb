{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Tue Nov 12 16:07:39 2019\n",
    "@author: Adam Syammas Zaki P\n",
    "\"\"\"\n",
    "# %%\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pybullet as pb\n",
    "import pybullet_data\n",
    "import cv2\n",
    "import random as r\n",
    "import time\n",
    "import load_object as lo \n",
    "import wrench_space_analysis as wsa\n",
    "import transformation as t\n",
    "import metrics as m \n",
    "import img_processor as ip\n",
    "import mask\n",
    "\n",
    "\"\"\"\n",
    "Note on parameters dictionary. This parameters describe the parameters belong to each specific part. \n",
    "The number below represents the index of the dictionary, and its respective description\n",
    "0. min grasp distance in pixel\n",
    "1. max grasp distance in pixel\n",
    "2. min grasp distance in local coordinate of respective part\n",
    "3. max grasp distance in local coordinate of respective part\n",
    "4. Image heigth for each grasp candidate from respective part\n",
    "5. Image width for each grasp candidate from respective part\n",
    "6. Total number of pixel when the respective part is not occluded, viewed fromt the camera\n",
    "7. The primitive geometry of the respective part\n",
    "8. Directory of .obj format file of the respective part\n",
    "9. Directory of .stl format file fo the respective part\n",
    "10. Directory for saving the synthetic training data for the respective part\n",
    "11. Directory for saving the synthetic training data for the respective part\n",
    "\"\"\"\n",
    "pi = np.pi/180\n",
    "parameters = {'hv18': [8, 12, 0.02, 0.025, 220, 100, 8000,\n",
    "                       'cylindrical',\n",
    "                       r'..\\..\\..\\data\\model\\hv18_2.obj', \n",
    "                       r'..\\..\\..\\data\\model\\hv18.stl',\n",
    "                       r'..\\..\\..\\Simulation\\Image\\Training Data\\Classification\\hv18\\Positive',\n",
    "                       r'..\\..\\..\\Simulation\\Image\\Training Data\\Classification\\hv18\\Negative'], \n",
    "              'hv8':[38, 44, 0.09, 0.11, 220, 220, 23000,\n",
    "                     'cicular',\n",
    "                     r'..\\..\\..\\data\\model\\hv8.obj', \n",
    "                     r'..\\..\\..\\data\\model\\hv8.stl',\n",
    "                     r'..\\..\\..\\Simulation\\Image\\Training Data\\Classification\\hv8\\Positive',\n",
    "                     r'..\\..\\..\\Simulation\\Image\\Training Data\\Classification\\hv8\\Negative']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "part_name = 'hv18'\n",
    "part_parameters = parameters[part_name]\n",
    "volumegrasp_range = [part_parameters[2], part_parameters[3]]    \n",
    "data_size = [part_parameters[4], part_parameters[5]]\n",
    "nonoccluded_pixel = part_parameters[6]\n",
    "geometry = part_parameters[7]\n",
    "part_dir = [part_parameters[8], part_parameters[9]]\n",
    "save_dir = [part_parameters[10], part_parameters[11]]\n",
    "\n",
    "    \n",
    "#loading local grasps and scores data\n",
    "local_grasps = np.load(r'../Npy File/local grasps/'+part_name+r'/local_grasps.npy')[0]\n",
    "grasp_scores = np.load(r'../Npy FIle/local grasps/'+part_name+r'/scores.npy')\n",
    "grasp_scores = 0.5*(grasp_scores[0]+grasp_scores[1])\n",
    "    \n",
    "#initializing rendering parameters\n",
    "far = 8.7\n",
    "aspect = 1.25\n",
    "near = 7.3\n",
    "fov = 37\n",
    "img_size = [1024, 1280]\n",
    "renderingParameters = [far, near, aspect, fov, img_size]\n",
    "    \n",
    "#opening simulation client\n",
    "physicsClient = pb.connect(pb.GUI)\n",
    " \n",
    "pb.setAdditionalSearchPath(pybullet_data.getDataPath())\n",
    "#standard unit in simulation is set to be dm\n",
    "pb.setGravity(0,0,-98) \n",
    "    \n",
    "#loading object\n",
    "pb.loadURDF('plane.urdf')\n",
    "lo.load_container()\n",
    "partID = []\n",
    "num_parts = np.random.randint(5, 25)\n",
    "   \n",
    "#simulation\n",
    "for i in range (20):\n",
    "    position = [0,0,i+2]\n",
    "    orientation = [r.uniform(-pi*360,pi*360), r.uniform(-pi*360,pi*360), r.uniform(-pi*360,pi*360)]\n",
    "    partID.append(lo.load_parts(position, orientation, part_dir))\n",
    "    \n",
    "for i in range (1000):\n",
    "    pb.stepSimulation(physicsClient)\n",
    "    time.sleep(1./240.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "viewMatrix = pb.computeViewMatrix(\n",
    "        cameraEyePosition=[0, 0, 9.1],\n",
    "        cameraTargetPosition=[0, 0, 0],\n",
    "        cameraUpVector=[0, 1, 0])\n",
    "            \n",
    "projectionMatrix = pb.computeProjectionMatrixFOV(\n",
    "                fov=fov,\n",
    "                aspect=aspect,\n",
    "                nearVal=near,\n",
    "                farVal=far)\n",
    "            \n",
    "width, height, rgbImg, depthbuff, segImg = pb.getCameraImage(\n",
    "                width=img_size[1], \n",
    "                height=img_size[0],\n",
    "                viewMatrix=viewMatrix,\n",
    "                projectionMatrix=projectionMatrix)\n",
    "\n",
    "viewMatrix = np.reshape(viewMatrix, (4,4), 'F')\n",
    "projectionMatrix = np.reshape(projectionMatrix, (4,4), 'F')\n",
    "rendering_matrices = [viewMatrix, projectionMatrix]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib qt\n",
    "\n",
    "plt.imshow(rgbImg)\n",
    "plt.show()\n",
    "\n",
    "idx = np.where(segImg == 2)\n",
    "nidx = np.where(segImg != 2)\n",
    "segImg[idx] = 1\n",
    "segImg[nidx] = 0\n",
    "\n",
    "plt.imshow(segImg, cmap = 'gray')\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(depthbuff,cmap='gray')\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(rgbImg)\n",
    "plt.show()\n",
    "\n",
    "pb.changeVisualShape(3, -1, rgbaColor=[0,0,0,0])\n",
    "\n",
    "viewMatrix = pb.computeViewMatrix(\n",
    "        cameraEyePosition=[0, 0, 9.1],\n",
    "        cameraTargetPosition=[0, 0, 0],\n",
    "        cameraUpVector=[0, 1, 0])\n",
    "            \n",
    "projectionMatrix = pb.computeProjectionMatrixFOV(\n",
    "                fov=fov,\n",
    "                aspect=aspect,\n",
    "                nearVal=near,\n",
    "                farVal=far)\n",
    "            \n",
    "width, height, rgbImg, depthbuff, segImg0 = pb.getCameraImage(\n",
    "                width=img_size[1], \n",
    "                height=img_size[0],\n",
    "                viewMatrix=viewMatrix,\n",
    "                projectionMatrix=projectionMatrix)\n",
    "\n",
    "viewMatrix = np.reshape(viewMatrix, (4,4), 'F')\n",
    "projectionMatrix = np.reshape(projectionMatrix, (4,4), 'F')\n",
    "rendering_matrices = [viewMatrix, projectionMatrix]\n",
    "\n",
    "idx = np.where(segImg0 == 2)\n",
    "nidx = np.where(segImg0 != 2)\n",
    "segImg0[idx] = 1\n",
    "segImg0[nidx] = 0\n",
    "\n",
    "plt.imshow(segImg0, cmap = 'gray')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\KosugeLab\\anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3335: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\Users\\KosugeLab\\anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:161: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "p_grasps = [] #projected grasps in an image\n",
    "u_grasps = [] #transformed grasps in universal coordinate\n",
    "fc_scores = [] #ferrary canny grasp quality metrics\n",
    "graspID = []  # Associated object id with each grasp candidates\n",
    "u_zvec = [] #transformed grasp approaching p\n",
    "p_zvec = [] #projected z_vector\n",
    "    \n",
    "\n",
    "for i, ID in enumerate(partID):\n",
    "    position, orientation = pb.getBasePositionAndOrientation(ID)\n",
    "    rot_matrix = np.reshape(pb.getMatrixFromQuaternion(orientation), [3,3])\n",
    "    T_matrix = np.zeros((4,4))\n",
    "    T_matrix[3,3] = 1\n",
    "    T_matrix[0:3,0:3] = rot_matrix\n",
    "    T_matrix[0:3,3] = position\n",
    "\n",
    "    #sampling grasps for each object\n",
    "    #each object will have 25 grasp candidates\n",
    "    num_sample = 1\n",
    "    indices = np.random.randint(0, len(local_grasps), size = num_sample)\n",
    "    sampled_grasps = local_grasps[indices]\n",
    "    sampled_graspscores = grasp_scores[indices]\n",
    "\n",
    "    #generate mask for orientation estimation\n",
    "    z_vec = mask.z_vec_transformation(rot_matrix, \n",
    "                                                   t.grasp_univ_transformation(T_matrix, sampled_grasps))\n",
    "    z_vec = mask.to_pixel(z_vec, rendering_matrices, img_size)\n",
    "\n",
    "    #grasps projection to an image\n",
    "    u_grasps.append(t.grasp_univ_transformation(T_matrix, sampled_grasps))\n",
    "\n",
    "    #remove clipped grasp candidates\n",
    "    projected, clipped = t.to_pixel(u_grasps[i], rendering_matrices, img_size)\n",
    "    sampled_graspscores = np.delete(sampled_graspscores, clipped, axis = 0)\n",
    "    z_vec = np.delete(z_vec, clipped, axis = 0)\n",
    "\n",
    "    p_grasps.append(projected)\n",
    "    fc_scores.append(sampled_graspscores)\n",
    "    graspID.append(np.broadcast_to(ID, (len(projected),)))\n",
    "    p_zvec.append(z_vec)\n",
    "        \n",
    "#reshaping tensor\n",
    "u_grasps = np.vstack(u_grasps)\n",
    "p_grasps = np.vstack(p_grasps)\n",
    "fc_scores = np.hstack(fc_scores)\n",
    "graspID = np.hstack(graspID)\n",
    "p_zvec = np.vstack(p_zvec)\n",
    "\n",
    "#processing depth image\n",
    "depthmap = t.to_depthMap(depthbuff, renderingParameters)\n",
    "depthImg = cv2.normalize(depthmap, \n",
    "                            dst=None, \n",
    "                            alpha=0, \n",
    "                            beta=255, \n",
    "                            norm_type=cv2.NORM_MINMAX, \n",
    "                            dtype=cv2.CV_8UC1)\n",
    "   \n",
    "    \n",
    "#collision estimation\n",
    "c_scores, debugging_cs = m.collision_est(depthmap, \n",
    "                               segImg, \n",
    "                                graspID, \n",
    "                                p_grasps,\n",
    "                                data_size)\n",
    "\n",
    "#occlusion rate estimation\n",
    "o_scores = m.occlusion_rate(segImg, \n",
    "                             graspID, \n",
    "                                p_grasps, \n",
    "                                data_size, \n",
    "                                nonoccluded_pixel)\n",
    "\n",
    "scores = np.array([fc_scores, o_scores, c_scores]).T\n",
    "\n",
    "\n",
    "grasp_img, grasp_bb, r_matrix = ip.data_generator(p_grasps,\n",
    "                      p_zvec,\n",
    "                     depthImg,\n",
    "                     scores,\n",
    "                     data_size)\n",
    "\n",
    "#creating mask\n",
    "viz, vect, v_mat, s_mat, invalid_indices = mask.mask_visualization(p_zvec,\n",
    "                                   p_grasps,\n",
    "                                   data_size,\n",
    "                                   save_dir)\n",
    "\n",
    "#pruning images\n",
    "scores = np.delete(scores, invalid_indices, axis = 0)\n",
    "grasp_img = np.delete(grasp_img, invalid_indices, axis = 0)\n",
    "grasp_bb = np.delete(grasp_bb, invalid_indices, axis = 0)\n",
    "debugging_cs = np.delete(debugging_cs, invalid_indices, axis = 0)\n",
    "p_grasps = np.delete(p_grasps, invalid_indices, axis = 0)\n",
    "u_grasps = np.delete(u_grasps, invalid_indices, axis = 0)\n",
    "p_zvec = np.delete(p_zvec, invalid_indices, axis = 0)\n",
    "\n",
    "visualizing = True\n",
    "#visualizing grasps\n",
    "if(visualize==True):\n",
    "    t.draw_realgrasps(u_grasps, fc_scores)\n",
    "\n",
    "#label grasp candidates\n",
    "labels = np.zeros(len(scores))\n",
    "p_index = np.where((scores[:,0]>0.7) &\n",
    "                   (scores[:,1]>0.9) &\n",
    "                   (scores[:,2]==0))\n",
    "labels[p_index] = 1\n",
    "\n",
    "full_grasp_img = ip.draw_grasp_representation(p_grasps, p_zvec, labels, depthImg, data_size)\n",
    "line_grasp_img = ip.draw_grasps(grasps, depthImg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib qt\n",
    "plt.imshow(full_grasp_img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image, ImageDraw\n",
    "import random\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import mask \n",
    "\n",
    "def draw_grasp_representation(grasps, ap_vectors, labels, depthImg, data_size):\n",
    "    '''\n",
    "    Draw grasp rectangle at the best grasp candidate's position\n",
    "    \n",
    "    #best_grasp : best grasp candidate\n",
    "                  Shape (2,2)\n",
    "    depthImg    : depth image\n",
    "    data_size   : the size of training data image we want to create.\n",
    "                  the size will depend on the size of the object\n",
    "    save_dir    : directory for saving the best grasps\n",
    "    '''\n",
    "    height = data_size[0]\n",
    "    width = data_size[1]\n",
    "    img_height, img_width = depthImg.shape\n",
    "    depthImg = cv2.cvtColor(depthImg, cv2.COLOR_GRAY2RGB)\n",
    "    dummy = Image.fromarray(depthImg)\n",
    "\n",
    "    for i in range(len(grasps)):\n",
    "        grasp = grasps[i]\n",
    "        ap_vector = ap_vectors[i]\n",
    "        label = labels[i]      \n",
    "        #rotating the grasp representation\n",
    "        center = ((grasp[0] + grasp[1]) / 2).astype(int)\n",
    "        vect1 = grasp[1] - grasp[0]\n",
    "        vect2 = grasp[0] - grasp[1]\n",
    "        angle1 = np.arctan2(vect1[1], vect1[0]) * 180 / np.pi\n",
    "        angle2 = np.arctan2(vect2[1], vect2[0]) * 180 / np.pi\n",
    "        angle = np.array([angle1, angle2]) \n",
    "        indices = np.argmin(np.abs(angle))\n",
    "        angle = angle[indices]\n",
    "        M = cv2.getRotationMatrix2D((center[0], center[1]), -angle, 1.0)\n",
    "\n",
    "        #defining image size and position\n",
    "        left = int(center[0] - width/2)\n",
    "        right = int(center[0] + width/2)\n",
    "        top = int(center[1] - 20)\n",
    "        bottom = int(center[1] + 20)\n",
    "        vertices = ((left,top,1), (left,bottom,1), (right,bottom,1), (right, top,1))\n",
    "        r_vertices = [M.dot(p) for p in vertices]\n",
    "        r_vertices = tuple(map(tuple, r_vertices))\n",
    "\n",
    "        #drawing grasp bounding box\n",
    "        draw = ImageDraw.Draw(dummy)\n",
    "    \n",
    "        draw.polygon(r_vertices, outline = (0,0,255))\n",
    "        #draw.ellipse(((ap_vector[0]-3, ap_vector[1]-3), (ap_vector[0]+3, ap_vector[1]+3)), fill =(0,0,255))\n",
    "\n",
    "    \n",
    "    #save image\n",
    "    return np.array(dummy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_grasp_img = draw_grasp_representation(p_grasps[p_index], p_zvec[p_index], labels[p_index], depthImg, data_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib qt\n",
    "plt.imshow(full_grasp_img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
